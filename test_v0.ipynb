{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a7dd05-99a4-402f-ad01-cd0114303900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T08:15:17.775680Z",
     "iopub.status.busy": "2025-01-20T08:15:17.775216Z",
     "iopub.status.idle": "2025-01-20T08:15:17.779810Z",
     "shell.execute_reply": "2025-01-20T08:15:17.779063Z",
     "shell.execute_reply.started": "2025-01-20T08:15:17.775645Z"
    },
    "tags": []
   },
   "source": [
    "## 简单说明\n",
    "\n",
    "v0版本，实现了基于 `Qwen2.5-Coder-7B-Instruct` 的 css 和 js 生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19ce4d0-e316-4af9-b907-db6aa4c3a30f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-20T08:15:44.915957Z",
     "iopub.status.busy": "2025-01-20T08:15:44.915679Z",
     "iopub.status.idle": "2025-01-20T08:15:53.413156Z",
     "shell.execute_reply": "2025-01-20T08:15:53.412646Z",
     "shell.execute_reply.started": "2025-01-20T08:15:44.915934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:15:50.644980: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-20 16:15:50.705498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 16:15:51.554458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import gradio as gr\n",
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "os.environ['GRADIO_ROOT_PATH'] = f\"/{os.environ['JUPYTER_NAME']}/proxy/7860\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66da6ad5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-20T08:15:53.414716Z",
     "iopub.status.busy": "2025-01-20T08:15:53.414173Z",
     "iopub.status.idle": "2025-01-20T08:15:53.421861Z",
     "shell.execute_reply": "2025-01-20T08:15:53.421413Z",
     "shell.execute_reply.started": "2025-01-20T08:15:53.414689Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_prefix = \"/mnt/workspace\"\n",
    "config_path = os.path.join(file_prefix, \"config.yaml\")\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as config_file:\n",
    "    config = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81c0c31-0c84-4975-a83a-4824420a85ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T08:15:53.422552Z",
     "iopub.status.busy": "2025-01-20T08:15:53.422345Z",
     "iopub.status.idle": "2025-01-20T08:15:53.425478Z",
     "shell.execute_reply": "2025-01-20T08:15:53.424949Z",
     "shell.execute_reply.started": "2025-01-20T08:15:53.422532Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_response(pattern, response: str) -> str:\n",
    "    match = re.search(pattern, response, re.S)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        print(\"c Error: Response error from LLM.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79173b9b-f4d0-4e3f-8b75-97cd2d97470f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-20T08:15:53.426290Z",
     "iopub.status.busy": "2025-01-20T08:15:53.426067Z",
     "iopub.status.idle": "2025-01-20T08:16:28.849908Z",
     "shell.execute_reply": "2025-01-20T08:16:28.848921Z",
     "shell.execute_reply.started": "2025-01-20T08:15:53.426270Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:15:54,112 - modelscope - INFO - Creating symbolic link [/mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct].\n",
      "2025-01-20 16:15:54,113 - modelscope - WARNING - Failed to create symbolic link /mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct for /mnt/workspace/.cache/modelscope/Qwen/Qwen2___5-Coder-7B-Instruct.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a47f487c942348ee2fc93b60f9b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:16:28,519 - modelscope - INFO - Creating symbolic link [/mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct].\n",
      "2025-01-20 16:16:28,519 - modelscope - WARNING - Failed to create symbolic link /mnt/workspace/.cache/modelscope/Qwen/Qwen2.5-Coder-7B-Instruct for /mnt/workspace/.cache/modelscope/Qwen/Qwen2___5-Coder-7B-Instruct.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "componentor = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def get_componentor_prompt(file):\n",
    "    system_content = config[\"role\"][\"componentor\"][\"system_content\"]\n",
    "    with open(os.path.join(file_prefix, \"prompt_files/demo0.html\"), 'r', encoding='utf-8') as f:\n",
    "        demo_source_code = f.read()\n",
    "    with open(os.path.join(file_prefix, \"prompt_files/demo0.js\"), 'r', encoding='utf-8') as f:\n",
    "        demo_js = f.read()\n",
    "    with open(os.path.join(file_prefix, \"prompt_files/demo0.css\"), 'r', encoding='utf-8') as f:\n",
    "        demo_css = f.read()\n",
    "    demo_prompt = config[\"role\"][\"componentor\"][\"example\"].format(demo_source_code=demo_source_code, demo_css=demo_css, demo_js=demo_js)\n",
    "    system_content = system_content + \"\\n\\n\" + demo_prompt\n",
    "    user_content = config[\"role\"][\"componentor\"][\"user_content\"]\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_code = f.read()\n",
    "    user_content = user_content.format(source_code=source_code)\n",
    "\n",
    "    return system_content, user_content\n",
    "\n",
    "\n",
    "# 处理上传文件并生成代码\n",
    "def generate_css_and_js(image, file):\n",
    "    # 构造 LLM 输入\n",
    "    system_content, user_content = get_componentor_prompt(file)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # 生成 组件化 代码\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(componentor.device)\n",
    "\n",
    "    generated_ids = componentor.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=8192\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # 解析大模型输出，拆分成 js + css\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    css = parse_response(pattern=r\"\\[\\[## css ##\\]\\]\\n(.*?)\\n\\[\\[## js ##\\]\\]\", response=response)\n",
    "    js = parse_response(pattern=r\"\\[\\[## js ##\\]\\]\\n(.*?)\\n\\[\\[## completed ##\\]\\]\", response=response)\n",
    "    return css, js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6b0f7f-b2d7-491d-aff2-03d14f9738bf",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-20T08:16:28.851174Z",
     "iopub.status.busy": "2025-01-20T08:16:28.850902Z",
     "iopub.status.idle": "2025-01-20T08:16:29.212076Z",
     "shell.execute_reply": "2025-01-20T08:16:29.211594Z",
     "shell.execute_reply.started": "2025-01-20T08:16:28.851144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1594, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 869, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7789/2095880335.py\", line 45, in generate_css_and_js\n",
      "    generated_ids = componentor.generate(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2252, in generate\n",
      "    result = self._sample(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3251, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1165, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 895, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 638, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 223, in forward\n",
      "    return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU \n"
     ]
    }
   ],
   "source": [
    "# 构建 Gradio 界面\n",
    "with gr.Blocks(css=\".file-input { height: 30px !important; }\") as demo:\n",
    "    gr.Markdown(\"<center><h1>组件复制门</h1></center>\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### 输入您想提取的组件截图和源码\")\n",
    "            image_input = gr.Image(label=\"上传图片(png文件)\", type=\"filepath\")\n",
    "            file_input = gr.File(label=\"上传文件\", interactive=True, container=False,\n",
    "                                 elem_classes=\"file-input\")\n",
    "            submit_btn = gr.Button(\"提交\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### 封装组件表现效果\")\n",
    "            # python_output = gr.Textbox(label=\"Python 运行结果\", interactive=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 封装的 JS 代码\")\n",
    "            generated_ans_js = gr.Code(label=\"JavaScript 代码\", language=\"javascript\")\n",
    "\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"### 封装的 CSS 代码\")\n",
    "            generated_ans_css = gr.Code(label=\"CSS 代码\", language=\"css\")\n",
    "\n",
    "    # 绑定交互逻辑\n",
    "    submit_btn.click(generate_css_and_js, inputs=[image_input, file_input],\n",
    "                     outputs=[generated_ans_css, generated_ans_js])\n",
    "\n",
    "# 运行 Gradio 应用\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
